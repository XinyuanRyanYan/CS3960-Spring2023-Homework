{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2abc7a41",
   "metadata": {},
   "source": [
    "# Homework 3: Discover, Measure, and Mitigate Bias in Bank Marketing\n",
    "\n",
    "## Background\n",
    "\n",
    "In this homework, we use a data coming from a bank’s marketing campaign. It consists of several individual level variables like age, gender, credit default, job etc., which can serve as input variables in the prediction model. The outcome varaible that the bank is interested in is whether a person subscribed to the term deposit or not. Hence, the outcome variable is categorical in nature ‐ subscribed or did not subscribe. The objective of training a model is to predict if someone would subscribe to the term deposit oﬀered by the bank or not. Given that the cost and time to contact all possible leads is enormous hence, ﬁnancial institutions like to identify the most promising leads. Promising leads are likely to be identiﬁed as proﬁle of people who are most likely to subscribe to a term deposit. Once identiﬁed, these leads are contacted through direct marketing channels (e.g., phone calls), they are provided with all the details about the term deposit.\n",
    "\n",
    "But the bank also wants to make sure that the prediction model is not biased against any group. They are cognizant that a prediction model built on prior data set has the potential to display bias against diﬀerent groups which precludes them from appearing in the list of promising leads. Considering that term deposits can help secure ﬁnancial stability in the long term, a biased prediction model can adversely aﬀect some groups. For the purpose of this project, we will consider marital status (married, not married) as the protected variable of interest. We will refer to the married people as the privileged group and examine whether there is diﬀerences in the privileged group versus the unprivileged group.\n",
    "\n",
    "| Protected Variable|Privileged Group|Unprivileged Group|\n",
    "| ----------------- | -------------- | ---------------- |\n",
    "|   Marital status  |     Married    |    Unmarried     |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d005adf",
   "metadata": {},
   "source": [
    "## Data Description\n",
    "The dataset consists of $5000$ rows and $12$ kinds of variables. Run the code below to look at the first $100$ instances of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c87811",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "bank_data = pd.read_csv('bank.csv')\n",
    "bank_data.head(n=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba5ca5b",
   "metadata": {},
   "source": [
    "In this data, we refer to the first 11 variables as **input variables** that are the input of the predicton model. The **outcome variable** of $subscribed$ denotes if the client has subscribed to a term deposit. For ease of explanation, we will refer to the two classes of the outcome variable as yes versus no indicating whether a person subscribed (yes) or did not subscribe (no). All variables are:\n",
    "\n",
    "* $age$: How old this client is. \n",
    "* $job$: Type of job. \n",
    "* $marital$: Marital status.\n",
    "* $education$: Highest education.\n",
    "* $default$: Has credit in default.\n",
    "* $housing$: Has housing loan?\n",
    "* $loan$: Has personal loan? \n",
    "* $contact$: Contact communication type.\n",
    "* $month$: Last contact month of year.\n",
    "* $day\\_of\\_week$: Last contact day of the week.\n",
    "* $duration$: Last contact duration, in seconds.\n",
    "* $subscribed$: Has the client subscribed a term deposit？ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df0d202",
   "metadata": {},
   "source": [
    "## Machine Learning Pipeline\n",
    "\n",
    "\n",
    "![image](../Images/MLWorkflow.png)\n",
    "We will use the bank data to review basics of how a predicted model is created in a supervised machine learning process.\n",
    "\n",
    "First, we split the initial dataset into training and test datasets with a random partitioning. Second, a machine learning algorithm is trained on this training dataset to produce a machine learning model. This generated model can be used to make a prediction when given a new instance. Next, the test dataset is used to assess the metrics the model, such as accuracy and fairness.\n",
    "\n",
    "Bias can enter the system in any of the three steps above. The training data set may be biased in that its outcome variable may be biased towards particular group of instances. The algorithm that creates the model may be biased in that it may generate models that are weighted towards particular variables in the input. The test data set may be biased in that it has expectations on correct answers that may be biased. Corresponding to the three kinds of the source of bias, researchers proposed three types of techniques to mitigate the bias: Pre-processing, In-processing, and Post-processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9f7f72",
   "metadata": {},
   "source": [
    "## Steps to Discover, Measure, and Mitigate Bias\n",
    "\n",
    "We are now ready to build a prediction model on the bank dataset, then discover, measure, and mitigate bias against the unmarried group in this model.\n",
    "\n",
    "First, we set the protected attribute as $marital$, with \"Married\" and \"Unmarried\" being the values for the privileged and unprivileged groups, respectively. Second, we split the Bank Marketing data into training data and test data with a specific split ratio. For example, a split ratio $0.7$ means that a random $70$ of the data goes to the training data, and the remaining becomes the test data. Third, we will build a prediction model on the training data using the Logistic Regression algorithm. Fourth, we get the prediction with the model on test data and check the accuracy and fairness of the prediction; we refer to them as the **baseline** as we don't apply any debiasing techniques so far. Fifth, we will apply Pre-processing, In-processing, and Post-processing techniques to mitigate biases in prediction, then check the accuracy and fairness of the debiased prediction again and compare them with the baseline. Finally, we will combine different debiasing techniques.\n",
    "\n",
    "Here are the steps involved:\n",
    "\n",
    "**1. Import libraries.**\n",
    "\n",
    "**2. Specify protected variable, privileged group, and unprivileged group.**\n",
    "\n",
    "**3. Split the Bank Marketing data into training data and test data.**\n",
    "\n",
    "**4. Build a prediction model.**\n",
    "\n",
    "    4.1 Train a Logistic Regression model using the training data.\n",
    "    4.2 Make prediction of the test data with the trained model.\n",
    "    4.3 Check fairness metrics and accuracy of the predition. (Baseline)\n",
    "    \n",
    "**5. Apply debiasing techniques to mitigate biases in prediction.**\n",
    "\n",
    "    5.1 Pre-processing techniques\n",
    "    5.2 In-processing techniques\n",
    "    5.3 Post-processing techniques\n",
    "**6. Flexibly combine different debiasing techniques to mitigate biases in prediction.**\n",
    "\n",
    "**7. Answer questions**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7ca2ab",
   "metadata": {},
   "source": [
    "### Step 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19df39b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aif360.datasets import BankDataset\n",
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from aif360.metrics import ClassificationMetric\n",
    "from aif360.algorithms.preprocessing import Reweighing\n",
    "from aif360.algorithms.preprocessing import LFR\n",
    "from aif360.algorithms.inprocessing import PrejudiceRemover\n",
    "from aif360.algorithms.postprocessing import RejectOptionClassification\n",
    "from IPython.display import Markdown, display\n",
    "print('If there are warnings, please ignore them.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ef5eb6",
   "metadata": {},
   "source": [
    "### Step 2. Load the bank data, specify protected variable, privileged group, and unprivileged group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fdccbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "protected_attribute_maps = [{1.0: 'married', 0.0: 'unmarried'}]\n",
    "dataset_orig = BankDataset(    # load the bank data\n",
    "            protected_attribute_names=['marital'],    # set the protected attribute as 'marital'\n",
    "            privileged_classes=[['married']],    # set 'married' as privileged group,\n",
    "                                                 # 'unmarried' will be unprivileged group automatically\n",
    "            features_to_drop=['campaign', 'pdays', 'previous', 'poutcome', 'emp.var.rate', 'cons.price.idx', 'cons.conf.idx', 'euribor3m', 'nr.employed'],\n",
    "            categorical_features=['job', 'education', 'default',\n",
    "                    'housing', 'loan', 'contact', 'month', 'day_of_week'],\n",
    "            metadata={'protected_attribute_maps': protected_attribute_maps}\n",
    "        )\n",
    "privileged_groups = [{'marital': 1}]\n",
    "unprivileged_groups = [{'marital': 0}]\n",
    "\n",
    "display(Markdown(\"#### The total number of instances in the bank data\"))\n",
    "print(dataset_orig.features.shape[0])\n",
    "display(Markdown(\"#### Protected variable name\"))\n",
    "print(dataset_orig.protected_attribute_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d928df05",
   "metadata": {},
   "source": [
    "### Step 3. Split the Bank Marketing data into training data and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fb6cda",
   "metadata": {},
   "source": [
    "We set the split ratio as $0.7$ by default, meaning that $70\\%$ instances in the dataset become the training data, and the remaining $30\\%$ instances become the test data. You can try different split ratio by changing the value of `split_ratio`, but we recommand $0.6-0.8$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7ee0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_ratio = 0.7\n",
    "dataset_orig_train, dataset_orig_test = dataset_orig.split([split_ratio], shuffle=None)\n",
    "display(Markdown(\"#### Split results\"))\n",
    "print(\"Split ratio = %f\" % split_ratio)\n",
    "print(\"The total number of instances in the bank data = %d\" % dataset_orig.features.shape[0])\n",
    "print(\"The number of instances in the training data = %d\" % dataset_orig_train.features.shape[0])\n",
    "print(\"The number of instances in the test data = %d\" % dataset_orig_test.features.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4968c6",
   "metadata": {},
   "source": [
    "### Step 4. Build a prediction model\n",
    "\n",
    "#### 4.1 & 4.2  Train a Logistic Regression model using the training data;  Make prediction of the test data with the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e427060",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Logistic_Regression(training_data, test_data):\n",
    "    model = LogisticRegression(random_state=0, max_iter = 1000)\n",
    "    # train the Logistic Regression model using the training data\n",
    "    model.fit(training_data.features, training_data.labels.ravel(), training_data.instance_weights)\n",
    "    # get the prediction on the test data using the model\n",
    "    prediction_label = model.predict(test_data.features)\n",
    "    prediction = dataset_orig_test.copy()\n",
    "    prediction.labels = prediction_label\n",
    "    # return the prediction\n",
    "    return prediction\n",
    "\n",
    "# get the baseline prediction of test data on the Logistic Regression model trained by the training data\n",
    "baseline_prediction = Logistic_Regression(dataset_orig_train, dataset_orig_test)\n",
    "\n",
    "display(Markdown(\"#### Prediction result of each instance in the test data\"))\n",
    "print(baseline_prediction.labels)\n",
    "print()\n",
    "print('Note: 0: The client subscribed a term deposit; 1: The client doesn\\'t subscribed a term deposit')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7652e3",
   "metadata": {},
   "source": [
    "#### 4.3 Check fairness metrics and accuracy of the predition. (Baseline)\n",
    "Then, we will check the accuracy of the prediction, and measure the fairness of the prediction using four fairness metrics: **Disparate impact**, **Statistical Parity Difference**, **Equal Opportunity Difference**, and **Average Odds Difference**. Please keep in mind that we will treat values of fairness metrics and accuracy as our baseline, because we haven't applied any debiasing techniques so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8def1e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# measure the accuracy and the fairness metrics on the prediction\n",
    "def get_prediction_metrics(prediction):\n",
    "    metric = ClassificationMetric(\n",
    "                        dataset_orig_test, prediction,\n",
    "                        unprivileged_groups=unprivileged_groups,\n",
    "                        privileged_groups=privileged_groups)\n",
    "    \n",
    "    print(\"Accuracy = %s\" % round(metric.accuracy(), 3))\n",
    "    print(\"Fairness metrics:\")\n",
    "    print(\"Disparate Impact = %s\" % round(metric.disparate_impact(), 3))\n",
    "    print(\"Statistical Parity Difference = %s\" % round(metric.statistical_parity_difference(), 3))\n",
    "    print(\"Equal Opportunity Difference = %s\" % round(metric.equal_opportunity_difference(), 3))\n",
    "    print(\"Average Odds Difference = %s\" % round(metric.average_odds_difference(), 3))\n",
    "\n",
    "display(Markdown(\"#### Baseline of accuracy and the fairness metrics on the prediction\"))\n",
    "\n",
    "# measure the accuracy and fairness of the prediction\n",
    "get_prediction_metrics(baseline_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167b17b3",
   "metadata": {},
   "source": [
    "### Step 5. Apply debiasing techniques to mitigate biases in prediction.\n",
    "\n",
    "In this step, we will try three types of debiasing techniques to mitigate biases: **Pre-processing**, **In-processing**, and **Post-processing**. For each type, we will use 1-2 methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65bd4ee",
   "metadata": {},
   "source": [
    "#### 5.1 Pre-processing\n",
    "Pre‐processing methods attempt to correct the bias by assigning appropriate weights to data points or by transforming them in such a way that discrimination can be reduced. The main idea of pre‐processing methods is to train a model on a “repaired” data set. In this section we will discuss two pre‐processing methods.\n",
    "\n",
    "**Pre-processing method 1: Reweighing method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75111e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing: reweighing method\n",
    "# This method requires the training data as input, then apply reweighing method on it, \n",
    "# returned the debiased training data.\n",
    "def Reweighing_method(training_data):\n",
    "    RW_model = Reweighing(unprivileged_groups=unprivileged_groups,\n",
    "            privileged_groups=privileged_groups)\n",
    "    return RW_model.fit_transform(training_data)\n",
    "\n",
    "# debiased training data using reweighing method\n",
    "dataset_RW_train = Reweighing_method(dataset_orig_train)\n",
    "\n",
    "# Train the Logistic Regression model with the debiased training data, then get the prediction on the test data\n",
    "prediction = Logistic_Regression(dataset_RW_train, dataset_orig_test)\n",
    "\n",
    "display(Markdown(\"#### Accuracy and the fairness metrics on the prediction of test data \\n * Training data: debiased training data using reweighing algorithm. \\n * Model: Logistic Regression model\"))\n",
    "\n",
    "# measure the accuracy and fairness of the prediction\n",
    "get_prediction_metrics(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2f3afb",
   "metadata": {},
   "source": [
    "**Pre-processing method 2: Learning fair representations (LFR)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa329a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing: Learning fair representations\n",
    "# This method requires the training data as input, then apply LFR method on it, \n",
    "# returned the debiased training data.\n",
    "def LFR_method(training_data):\n",
    "    LFR_model = LFR(unprivileged_groups=unprivileged_groups, \n",
    "    privileged_groups=privileged_groups,\n",
    "    verbose=0, seed=10)\n",
    "    LFR_model = LFR_model.fit(dataset_orig_train)\n",
    "    return LFR_model.transform(dataset_orig_train)\n",
    "\n",
    "print('please wait for the results, this process may take 20 senconds ...')\n",
    "\n",
    "# debiased training data using LFR method\n",
    "dataset_LFR_train = LFR_method(dataset_orig_train)\n",
    "\n",
    "# Train the Logistic Regression model with the debiased training data, then get the prediction on the test data\n",
    "prediction = Logistic_Regression(dataset_LFR_train, dataset_orig_test)\n",
    "\n",
    "display(Markdown(\"#### Accuracy and the fairness metrics on the prediction of test data \\n * Training data: debiased training data using Learning fair representations (LFR) algorithm. \\n * Model: Logistic Regression model\"))\n",
    "\n",
    "# measure the accuracy and fairness of the prediction\n",
    "get_prediction_metrics(prediction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a00047",
   "metadata": {},
   "source": [
    "#### 5.2 In-processing\n",
    "In‐processing debiasing occurs during the training process in which a method is attempting to learn the relationship between the input features and the outcome. The goal is to reduce the reliance on learning the relationship between protected attributes and the outcome.\n",
    "Put it short, In-processing methods just replace the original model with a new model that is fairer.\n",
    "\n",
    "**In-processing method 1: Prejudice Remover**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c055a7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in-processing: Prejudice remover\n",
    "# This method take the training data and test data as input\n",
    "# The training data is used to train the Prejudice remover model, \n",
    "# then return the prediction of the test data on the trained model.\n",
    "def Prejudice_Remover(training_data, test_data):\n",
    "    model = PrejudiceRemover(eta=0.1)\n",
    "    model.fit(training_data)\n",
    "    prediction = model.predict(test_data)\n",
    "    return prediction\n",
    "\n",
    "print('please wait for the results, this process may take 20 senconds ...')\n",
    "\n",
    "# Train Prejudice Remover model using the orginal training data, then return the predition on the original test data.\n",
    "prediction = Prejudice_Remover(dataset_orig_train, dataset_orig_test)\n",
    "\n",
    "display(Markdown(\"#### Accuracy and the fairness metrics on the prediction of test data \\n * Training data: original training data. \\n * Model: Prejudice Remover model\"))\n",
    "\n",
    "# measure the accuracy and fairness of the prediction\n",
    "get_prediction_metrics(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e77f80e",
   "metadata": {},
   "source": [
    "**In-processing method 2: Adversarial Debiasing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a643673b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in-processing: Adversarial debiasing\n",
    "from aif360.algorithms.inprocessing.adversarial_debiasing import AdversarialDebiasing\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_eager_execution()\n",
    "\n",
    "# This method take the training data and test data as input\n",
    "# The training data is used to train the Adversarial debiasing model, \n",
    "# then return the prediction of the test data on the trained model.\n",
    "def Adversarial_debiasing(training_data, test_data):\n",
    "    tf.reset_default_graph()\n",
    "    sess = tf.Session()\n",
    "    num_epochs = 50\n",
    "    classifier_num_hidden_units = 200\n",
    "    model = AdversarialDebiasing(privileged_groups = privileged_groups,\n",
    "                                unprivileged_groups = unprivileged_groups,\n",
    "                                scope_name='debiased_classifier',\n",
    "                                debias=True,\n",
    "                                sess=sess)\n",
    "    model.fit(training_data)\n",
    "    prediction = model.predict(test_data)\n",
    "    return prediction\n",
    "\n",
    "# Train Adversarial debiasing model using the orginal training data, then return the predition on the original test data.\n",
    "prediction = Adversarial_debiasing(dataset_orig_train, dataset_orig_test)\n",
    "\n",
    "display(Markdown(\"#### Accuracy and the fairness metrics on the prediction of test data \\n * Training data: original training data. \\n * Model: Adversarial debiasing\"))\n",
    "\n",
    "# measure the accuracy and fairness of the prediction\n",
    "get_prediction_metrics(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd252ca",
   "metadata": {},
   "source": [
    "#### Post-processing\n",
    "The post-processing approach mainly focus on modifying the prediction result to make it fairer. In this section, we will use one popular technique, called Reject Option‐based Classiﬁcation (ROC), to ajust the baseline prediction so that it become fairer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036a2ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post-processing: Reject Option‐based Classiﬁcation (ROC)\n",
    "\n",
    "def ROC(prediction):\n",
    "    model = RejectOptionClassification(privileged_groups = privileged_groups,\n",
    "                                    unprivileged_groups = unprivileged_groups, num_class_thresh=500)\n",
    "    model = model.fit(dataset_orig_test, prediction)\n",
    "    prediction = model.predict(prediction)\n",
    "    return prediction\n",
    "\n",
    "print('please wait for the results, this process may take 20 senconds ...')\n",
    "\n",
    "# get the results after applying Reject Option‐based Classiﬁcation to the prediction baseline\n",
    "prediction = ROC(baseline_prediction)\n",
    "\n",
    "display(Markdown(\"#### Accuracy and the fairness metrics of the prediction after applying Reject Option‐based Classiﬁcation\"))\n",
    "\n",
    "# measure the accuracy and fairness of the prediction\n",
    "get_prediction_metrics(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42989f94",
   "metadata": {},
   "source": [
    "### Step 6. Flexibly combine different debiasing techniques to mitigate biases.\n",
    "In **step 5**, we show the uses of debiasing techniques individually, including pre-processing, in-processing, and post-processing. In this step, we will show how to combine different debiasing techniques to mitigate baises. In the following code, \n",
    "* we apply the **Reweighing** (pre-processing) algorithm on the original training data, \n",
    "* and use the debiased training data to train a fairer model **Prejudice Remover** (in-processing), \n",
    "* then get the prediction of test data on the trained model, \n",
    "* finally, we use **Reject Option‐based Classiﬁcation** (post-processing) to debaised the prediction.\n",
    "* We evaluate the accuracy and fairness of the final prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c89f496",
   "metadata": {},
   "outputs": [],
   "source": [
    "### This code shows how to combine three debiasing methdods together: Reweighing, Prejudice Remover, and Reject Option‐based Classiﬁcation\n",
    "\n",
    "print('please wait for the results, this process may take 30 senconds ...')\n",
    "\n",
    "# 1. pre-processing: debias the original training data using Reweighing\n",
    "dataset_RW_train = Reweighing_method(dataset_orig_train)\n",
    "# 2. in-processing: train Prejudice Remover model using debiased training data, then get the prediction of test data\n",
    "prediction = Prejudice_Remover(dataset_RW_train, dataset_orig_test)\n",
    "# 3. post-processing: use Reject Option‐based Classiﬁcation to debaised the prediction\n",
    "final_prediction = ROC(prediction)\n",
    "\n",
    "display(Markdown(\"#### Accuracy and the fairness metrics of the prediction debaised by Reweighing, Prejudice Remover, and Reject Option‐based Classiﬁcation\"))\n",
    "# 4. evaluate the accuracy and fairness of the final prediction\n",
    "get_prediction_metrics(final_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39173fd4",
   "metadata": {},
   "source": [
    "### Step 7. Answer questions\n",
    "Please answer the following questions by editing or runing the code above.\n",
    "\n",
    "**Q1 (10 points)**: In **step 3**, change the value of the `split_ratio` variable (recommend 0.6-0.8), and run the code in **step 3** and **step 4** to build a Logistic Regression model and make predictions on the test data. Report the value of `split_ratio`, the accuracy, and the four fairness metrics of the predictions (baseline). \n",
    "\n",
    "**Q2 (20 points)**: Run the code in **step 5.1**. For each pre-processing method (*Reweighing method* and *Learning fair representations*), report the accuracy and four fairness metrics of the debiased predictions. Compare the results with the baseline generated in **Q1**, How do the accuracy and fairness metrics change？\n",
    "\n",
    "**Q3 (20 points)**: Run the code in **step 5.2**. For each in-processing method (*Prejudice Remover* and *Adversarial Debiasing*), report the accuracy and four fairness metrics of the debiased predictions. Compare the results with the baseline generated in **Q1**, How do the accuracy and fairness metrics change？\n",
    "\n",
    "**Q4 (10 points)**: Run the code in **step 5.3**, for the post-processing method (*Reject Option‐based Classiﬁcation*), report the accuracy and four fairness metrics of the debiased prediction. Compare the results with the baseline generated in **Q1**, How do the accuracy and fairness metrics change？\n",
    "\n",
    "**Q5 (30 points)**: Following **step 6**, choose two additional combinations of debiasing techniques. For each combination, report the combination, the code, the accuracy and four fairness metrics of the final prediction, and compare the accuracy and four fairness metrics with the baseline generated in **Q1**.\n",
    "*Reminder: For each combination, you should use at least two debiasing techniques.*\n",
    "\n",
    "**Q6 (10 points)**: Compare all accuracies and fairness metrics you generated from **Q1**-**Q5**, what do you find about the relationship between fairness and accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a07ee2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HWfairAIVenv",
   "language": "python",
   "name": "hwfairaivenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
